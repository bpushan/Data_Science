{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d252dbbf-717c-4660-81db-0c458dd13447",
   "metadata": {},
   "source": [
    "# Application of Artificial Intelligence Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea1373f-f33a-4e5a-a7e2-b530f7d4f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1452d-5b73-4a08-a3cd-ddb6cb7f45ae",
   "metadata": {},
   "source": [
    "#### Setup the dataset and annotation directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e553a5-e82a-457e-b644-7a1cb3395bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "print(\"Base directory: \", base_dir)\n",
    "data_dir = os.path.join(base_dir, 'dataset2')\n",
    "print(\"Data directory: \", data_dir)\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "print(\"Image directory: \", image_dir)\n",
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "print(\"Annotations directory: \", annotations_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c7261-2177-484f-a331-8fc6df9483b9",
   "metadata": {},
   "source": [
    "#### Create the Test and Train set (100-28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2fa079-b68b-4cac-8c80-b27ff79b2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train (100) and test (28)\n",
    "all_images = os.listdir(image_dir)\n",
    "train_images = all_images[:100]\n",
    "test_images = all_images[100:]\n",
    "\n",
    "## annotation files will be read when the train-test images are processed. It is easier to tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e8cef-5e38-4518-a0ac-6005b26f8ec2",
   "metadata": {},
   "source": [
    "### Load and Transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbbb785-67df-417d-9877-8ede917e9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3828a-962e-4b35-bbe1-7ae14a5cffd2",
   "metadata": {},
   "source": [
    "#### Setting up a way to read the annotation file and set inline with COCO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e083d73-75b3-4df1-857c-d196d4dd2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': [[72, 150, 172, 399], [192, 229, 308, 534], [324, 246, 415, 410]], 'labels': [4, 3, 1]}\n"
     ]
    }
   ],
   "source": [
    "def parse_voc_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        if name == 'obj1':\n",
    "            label = 1\n",
    "        elif name == 'obj2':\n",
    "            label = 2\n",
    "        elif name == 'obj3':\n",
    "            label = 3\n",
    "        elif name == 'obj4':\n",
    "            label = 4\n",
    "        \n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        \n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "# Example usage\n",
    "example_path = os.path.join(annotations_dir, '33.xml')  ## has 3 objects\n",
    "parsed_annotation = parse_voc_annotation(example_path)\n",
    "print(parsed_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030c4b91-1c17-47f8-a3b9-a2e2339c7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, image_list, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_list = image_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.image_list[idx].replace('.jpg', '.xml'))\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        annot = parse_voc_annotation(annotation_path)\n",
    "        \n",
    "        boxes = torch.as_tensor(annot['boxes'], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(annot['labels'], dtype=torch.int64)\n",
    "        # Generate a unique image_id based on idx or filename\n",
    "        image_id = idx  # Example: using idx as image_id\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply transformations to the image only\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "# Transformations\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4709bc5-8bc3-43ae-9a80-423a7698414d",
   "metadata": {},
   "source": [
    "#### Creating the datasets and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ffd4f2-61a4-4c2f-9e1c-980f3c72f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CustomDataset(image_dir, annotations_dir, train_images, transform)\n",
    "test_dataset = CustomDataset(image_dir, annotations_dir, test_images, transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae81dfb-71e6-4474-8e1d-28fd8db2881e",
   "metadata": {},
   "source": [
    "#### Load the Pretrained Model\n",
    "Load a pretrained Faster R-CNN model and modify it to fit our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe14ba3-038b-4210-bd76-cf6de22be4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "num_classes = 5  # 4 classes (Water Bottle, Milk Bottle, Tetra Pack, Can) + background\n",
    "model = get_model_instance_segmentation(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248146bb-9829-44bd-9a88-96db4050fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting python files for torchvision and coco\n",
    "from engine_detection import train_one_epoch, evaluate\n",
    "import utils_detection as utils\n",
    "from coco_utils_detection import get_coco_api_from_dataset\n",
    "from coco_eval_detection import CocoEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328d4a5-3b20-4556-9c16-549b83af23da",
   "metadata": {},
   "source": [
    "#### Fine tuning the FastRCNN model with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e84808-9f51-4e9a-89e9-e988450cc416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/50]  eta: 0:43:50  lr: 0.000107  loss: 2.1990 (2.1990)  loss_classifier: 1.9727 (1.9727)  loss_box_reg: 0.2160 (0.2160)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0084 (0.0084)  time: 52.6128  data: 0.2190\n",
      "Epoch: [0]  [10/50]  eta: 0:34:36  lr: 0.001126  loss: 1.1907 (1.3750)  loss_classifier: 0.9729 (1.0818)  loss_box_reg: 0.2703 (0.2658)  loss_objectness: 0.0171 (0.0177)  loss_rpn_box_reg: 0.0084 (0.0097)  time: 51.9122  data: 0.2629\n",
      "Epoch: [0]  [20/50]  eta: 0:24:31  lr: 0.002146  loss: 0.7239 (1.0505)  loss_classifier: 0.4186 (0.7490)  loss_box_reg: 0.2703 (0.2711)  loss_objectness: 0.0099 (0.0214)  loss_rpn_box_reg: 0.0078 (0.0090)  time: 48.8638  data: 0.5069\n",
      "Epoch: [0]  [30/50]  eta: 0:15:07  lr: 0.003165  loss: 0.5157 (0.8722)  loss_classifier: 0.2851 (0.5799)  loss_box_reg: 0.2339 (0.2614)  loss_objectness: 0.0118 (0.0214)  loss_rpn_box_reg: 0.0092 (0.0094)  time: 41.7812  data: 0.9619\n",
      "Epoch: [0]  [40/50]  eta: 0:07:33  lr: 0.004184  loss: 0.4490 (0.7709)  loss_classifier: 0.1926 (0.4831)  loss_box_reg: 0.2339 (0.2597)  loss_objectness: 0.0147 (0.0194)  loss_rpn_box_reg: 0.0074 (0.0086)  time: 41.4558  data: 1.4657\n",
      "Epoch: [0]  [49/50]  eta: 0:00:44  lr: 0.005000  loss: 0.4653 (0.7249)  loss_classifier: 0.1941 (0.4366)  loss_box_reg: 0.2392 (0.2614)  loss_objectness: 0.0116 (0.0181)  loss_rpn_box_reg: 0.0068 (0.0088)  time: 42.6757  data: 1.0491\n",
      "Epoch: [0] Total time: 0:37:06 (44.5280 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/14]  eta: 0:04:48  model_time: 20.3255 (20.3255)  evaluator_time: 0.1977 (0.1977)  time: 20.6427  data: 0.1104\n",
      "Test:  [13/14]  eta: 0:00:19  model_time: 18.5998 (19.7767)  evaluator_time: 0.0469 (0.0889)  time: 19.9766  data: 0.1092\n",
      "Test: Total time: 0:04:39 (19.9766 s / it)\n",
      "Averaged stats: model_time: 18.5998 (19.7767)  evaluator_time: 0.0469 (0.0889)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.55s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.097\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.309\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.117\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.356\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.397\n",
      "Epoch: [1]  [ 0/50]  eta: 0:49:18  lr: 0.005000  loss: 0.3497 (0.3497)  loss_classifier: 0.1430 (0.1430)  loss_box_reg: 0.1992 (0.1992)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0023 (0.0023)  time: 59.1622  data: 0.3124\n",
      "Epoch: [1]  [10/50]  eta: 0:28:47  lr: 0.005000  loss: 0.5012 (0.4565)  loss_classifier: 0.2167 (0.2003)  loss_box_reg: 0.2575 (0.2375)  loss_objectness: 0.0093 (0.0104)  loss_rpn_box_reg: 0.0074 (0.0083)  time: 43.1762  data: 0.4456\n",
      "Epoch: [1]  [20/50]  eta: 0:20:04  lr: 0.005000  loss: 0.4736 (0.4491)  loss_classifier: 0.2127 (0.2051)  loss_box_reg: 0.2276 (0.2232)  loss_objectness: 0.0104 (0.0110)  loss_rpn_box_reg: 0.0100 (0.0098)  time: 39.2092  data: 0.3175\n",
      "Epoch: [1]  [30/50]  eta: 0:12:55  lr: 0.005000  loss: 0.4294 (0.4386)  loss_classifier: 0.2127 (0.2049)  loss_box_reg: 0.1996 (0.2151)  loss_objectness: 0.0064 (0.0094)  loss_rpn_box_reg: 0.0100 (0.0093)  time: 36.3186  data: 0.1203\n",
      "Epoch: [1]  [40/50]  eta: 0:06:22  lr: 0.005000  loss: 0.3923 (0.4252)  loss_classifier: 0.1915 (0.2003)  loss_box_reg: 0.1872 (0.2074)  loss_objectness: 0.0048 (0.0089)  loss_rpn_box_reg: 0.0076 (0.0085)  time: 36.1514  data: 0.1233\n",
      "Epoch: [1]  [49/50]  eta: 0:00:37  lr: 0.005000  loss: 0.3995 (0.4311)  loss_classifier: 0.1915 (0.2061)  loss_box_reg: 0.1932 (0.2079)  loss_objectness: 0.0048 (0.0089)  loss_rpn_box_reg: 0.0059 (0.0082)  time: 36.0761  data: 0.1180\n",
      "Epoch: [1] Total time: 0:31:28 (37.7630 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/14]  eta: 0:04:19  model_time: 18.4433 (18.4433)  evaluator_time: 0.0468 (0.0468)  time: 18.5213  data: 0.0312\n",
      "Test:  [13/14]  eta: 0:00:19  model_time: 19.7215 (19.5378)  evaluator_time: 0.0156 (0.0134)  time: 19.6409  data: 0.0898\n",
      "Test: Total time: 0:04:34 (19.6409 s / it)\n",
      "Averaged stats: model_time: 19.7215 (19.5378)  evaluator_time: 0.0156 (0.0134)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.08s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.221\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.380\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.254\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.248\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.481\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504\n",
      "Epoch: [2]  [ 0/50]  eta: 0:43:51  lr: 0.005000  loss: 0.2540 (0.2540)  loss_classifier: 0.1130 (0.1130)  loss_box_reg: 0.1163 (0.1163)  loss_objectness: 0.0205 (0.0205)  loss_rpn_box_reg: 0.0042 (0.0042)  time: 52.6373  data: 0.0781\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 20\u001b[0m     train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     21\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m     evaluate(model, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\Documents\\ISB AMPBA\\Residency 5\\Artificial Intelligence\\Assignment\\engine_detection.py:51\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[0;32m     49\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     52\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_classes = 5  # 4 classes + background\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, test_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
